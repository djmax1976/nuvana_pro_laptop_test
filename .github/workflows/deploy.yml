# Deploy Pipeline - Deployment Only (push-triggered)
# Trusts that CI passed on PR before merge
# IMPORTANT: Requires branch protection with required status checks enabled

name: Deploy

on:
  push:
    branches: [main]

permissions:
  contents: read
  id-token: write

env:
  NODE_VERSION: "20"
  AWS_REGION: us-east-1

concurrency:
  group: deploy-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel in-progress deployments

jobs:
  deploy:
    name: Deploy to AWS
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    steps:
      - name: Checkout code
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4.3.1

      - name: Determine environment
        id: set-env
        run: |
          if [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "environment=prod" >> $GITHUB_OUTPUT
          else
            echo "environment=dev" >> $GITHUB_OUTPUT
          fi

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020  # v4.4.0
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Install backend dependencies
        run: cd backend && npm ci

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502  # v4.0.2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get database URL from Secrets Manager
        id: get-db-url
        env:
          ENVIRONMENT: ${{ steps.set-env.outputs.environment }}
        run: |
          DB_URL=$(aws secretsmanager get-secret-value \
            --secret-id /nuvana-$ENVIRONMENT/database/url \
            --query 'SecretString' \
            --output text)
          echo "database_url=$DB_URL" >> $GITHUB_OUTPUT
          echo "✓ Retrieved database URL from Secrets Manager"

      - name: Generate Prisma Client
        run: cd backend && npx prisma generate
        env:
          DATABASE_URL: ${{ steps.get-db-url.outputs.database_url }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@062b18b96a7aff071d4dc91bc00c4c1a7945b076  # v2.0.1

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@b5ca514318bd6ebac0fb2aedd5d36ec1b5c232a2  # v3.10.0

      - name: Get ALB DNS name
        id: get-alb-dns
        env:
          ENVIRONMENT: ${{ steps.set-env.outputs.environment }}
        run: |
          ALB_DNS=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?contains(LoadBalancerName, 'nuvana-$ENVIRONMENT')].DNSName" \
            --output text)
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
          echo "ALB DNS: $ALB_DNS"

      - name: Build and push frontend image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: nuvana-${{ steps.set-env.outputs.environment }}-frontend
          IMAGE_TAG: ${{ github.sha }}
          ALB_DNS: ${{ steps.get-alb-dns.outputs.alb_dns }}
        run: |
          docker build \
            -f Dockerfile.frontend \
            --build-arg NEXT_PUBLIC_BACKEND_URL=http://$ALB_DNS \
            --build-arg NEXT_PUBLIC_API_URL=http://$ALB_DNS \
            -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG

      - name: Build and push backend image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: nuvana-${{ steps.set-env.outputs.environment }}-backend
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build \
            -f backend/Dockerfile \
            -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            ./backend
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG

      - name: Get ECS cluster and subnet info
        id: get-ecs-info
        env:
          ENVIRONMENT: ${{ steps.set-env.outputs.environment }}
        run: |
          CLUSTER_NAME="nuvana-$ENVIRONMENT-cluster"
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          
          # Get private subnet IDs from VPC
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=nuvana-$ENVIRONMENT-vpc" \
            --query 'Vpcs[0].VpcId' \
            --output text)
          
          # Get subnet IDs as space-separated list
          SUBNET_IDS=$(aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Name,Values=nuvana-$ENVIRONMENT-private-*" \
            --query 'Subnets[*].SubnetId' \
            --output text)
          
          # Store as space-separated (we'll convert to JSON in the migration step)
          echo "subnet_ids=$SUBNET_IDS" >> $GITHUB_OUTPUT
          echo "✓ Retrieved ECS cluster and subnet information"

      - name: Get security group ID
        id: get-sg-id
        env:
          ENVIRONMENT: ${{ steps.set-env.outputs.environment }}
        run: |
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=tag:Name,Values=nuvana-$ENVIRONMENT-ecs-tasks-sg" \
            --query 'SecurityGroups[0].GroupId' \
            --output text)
          echo "security_group_id=$SG_ID" >> $GITHUB_OUTPUT
          echo "✓ Retrieved ECS security group ID"

      - name: Run database migrations via ECS
        env:
          CLUSTER: ${{ steps.get-ecs-info.outputs.cluster_name }}
          TASK_DEFINITION: nuvana-${{ steps.set-env.outputs.environment }}-migration
          SUBNET_IDS: ${{ steps.get-ecs-info.outputs.subnet_ids }}
          SECURITY_GROUP_ID: ${{ steps.get-sg-id.outputs.security_group_id }}
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE: ${{ steps.login-ecr.outputs.registry }}/nuvana-${{ steps.set-env.outputs.environment }}-backend:${{ github.sha }}
        run: |
          echo "Running database migrations via ECS task..."

          # Convert tab/space-separated subnet IDs to JSON array (handle both tabs and spaces)
          # First normalize to spaces, then split and create JSON array
          SUBNET_IDS_NORMALIZED=$(echo "$SUBNET_IDS" | tr '\t' ' ' | xargs)
          SUBNET_IDS_ARRAY=$(echo "$SUBNET_IDS_NORMALIZED" | awk '{for(i=1;i<=NF;i++){printf "\"%s\"", $i; if(i<NF)printf ","}}' | awk '{printf "[%s]", $0}')

          # Create network configuration JSON
          NETWORK_CONFIG=$(echo "{\"awsvpcConfiguration\":{\"subnets\":$SUBNET_IDS_ARRAY,\"securityGroups\":[\"$SECURITY_GROUP_ID\"],\"assignPublicIp\":\"DISABLED\"}}")

          # Get the latest revision of the task definition
          # First, check if task definition exists by listing task definitions for this family
          echo "Checking for existing task definition: $TASK_DEFINITION"
          LATEST_TASK_DEF_ARN=$(aws ecs list-task-definitions \
            --family-prefix "$TASK_DEFINITION" \
            --sort DESC \
            --max-items 1 \
            --query 'taskDefinitionArns[0]' \
            --output text 2>/dev/null || echo "")

          if [ -z "$LATEST_TASK_DEF_ARN" ] || [ "$LATEST_TASK_DEF_ARN" == "None" ]; then
            echo "❌ Task definition $TASK_DEFINITION not found. Please create it first in AWS ECS."
            echo "You can create it using the AWS Console or CLI with a base configuration."
            exit 1
          fi

          echo "Found existing task definition: $LATEST_TASK_DEF_ARN"
          
          # Extract revision number from ARN (format: arn:aws:ecs:region:account:task-definition/family:revision)
          TASK_DEF_WITH_REVISION=$(echo "$LATEST_TASK_DEF_ARN" | awk -F'/' '{print $2}')

          # Update migration task definition with new image before running
          # Get current task definition and update the image
          echo "Describing task definition: $TASK_DEF_WITH_REVISION"
          aws ecs describe-task-definition --task-definition "$TASK_DEF_WITH_REVISION" --query 'taskDefinition' > migration-task-def.json

          # Update image in task definition and remove fields that can't be set
          jq ".containerDefinitions[0].image = \"$IMAGE\" | del(.taskDefinitionArn) | del(.revision) | del(.status) | del(.requiresAttributes) | del(.compatibilities) | del(.registeredAt) | del(.registeredBy)" migration-task-def.json > migration-task-def-updated.json

          # Register new task definition with updated image
          NEW_TASK_DEF=$(aws ecs register-task-definition --cli-input-json file://migration-task-def-updated.json --query 'taskDefinition.taskDefinitionArn' --output text)
          echo "Registered new migration task definition: $NEW_TASK_DEF"

          # Run the migration task with the updated task definition
          TASK_ARN=$(aws ecs run-task \
            --cluster $CLUSTER \
            --task-definition $NEW_TASK_DEF \
            --launch-type FARGATE \
            --network-configuration "$NETWORK_CONFIG" \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "Migration task ARN: $TASK_ARN"
          echo "Waiting for migration task to complete..."

          # Wait for task to complete (max 10 minutes)
          aws ecs wait tasks-stopped \
            --cluster $CLUSTER \
            --tasks $TASK_ARN \
            --max-attempts 60 \
            --delay 10 || true

          # Get task exit code
          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster $CLUSTER \
            --tasks $TASK_ARN \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)

          if [ "$EXIT_CODE" != "0" ] && [ "$EXIT_CODE" != "null" ]; then
            echo "❌ Migration failed with exit code: $EXIT_CODE"
            echo "Checking logs..."
            aws logs tail "/ecs/$CLUSTER/migration" --since 10m --format short || true
            exit 1
          else
            echo "✓ Database migrations completed successfully"
          fi

      # Note: RBAC seed is skipped in production as tsx is not available in production image
      # Seeds should be run manually or via a separate process that includes dev dependencies
      # - name: Seed RBAC roles and permissions (via ECS)

      - name: Update frontend ECS service
        env:
          CLUSTER: nuvana-${{ steps.set-env.outputs.environment }}-cluster
          SERVICE: nuvana-${{ steps.set-env.outputs.environment }}-frontend
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Get current task definition
          TASK_DEF=$(aws ecs describe-services --cluster $CLUSTER --services $SERVICE --query 'services[0].taskDefinition' --output text)
          
          # Get task definition JSON and update image
          NEW_IMAGE="$ECR_REGISTRY/nuvana-${{ steps.set-env.outputs.environment }}-frontend:$IMAGE_TAG"
          aws ecs describe-task-definition --task-definition $TASK_DEF --query 'taskDefinition' > task-def.json
          
          # Update image in task definition and remove fields that can't be set
          jq ".containerDefinitions[0].image = \"$NEW_IMAGE\" | del(.taskDefinitionArn) | del(.revision) | del(.status) | del(.requiresAttributes) | del(.compatibilities) | del(.registeredAt) | del(.registeredBy)" task-def.json > task-def-updated.json
          
          # Register new task definition
          NEW_TASK_DEF=$(aws ecs register-task-definition --cli-input-json file://task-def-updated.json --query 'taskDefinition.taskDefinitionArn' --output text)
          
          # Update service with new task definition
          aws ecs update-service \
            --cluster $CLUSTER \
            --service $SERVICE \
            --task-definition $NEW_TASK_DEF \
            --force-new-deployment

      - name: Update backend ECS service
        env:
          CLUSTER: nuvana-${{ steps.set-env.outputs.environment }}-cluster
          SERVICE: nuvana-${{ steps.set-env.outputs.environment }}-backend
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Get current task definition
          TASK_DEF=$(aws ecs describe-services --cluster $CLUSTER --services $SERVICE --query 'services[0].taskDefinition' --output text)
          
          # Get task definition JSON and update image
          NEW_IMAGE="$ECR_REGISTRY/nuvana-${{ steps.set-env.outputs.environment }}-backend:$IMAGE_TAG"
          aws ecs describe-task-definition --task-definition $TASK_DEF --query 'taskDefinition' > task-def.json
          
          # Update image in task definition and remove fields that can't be set
          jq ".containerDefinitions[0].image = \"$NEW_IMAGE\" | del(.taskDefinitionArn) | del(.revision) | del(.status) | del(.requiresAttributes) | del(.compatibilities) | del(.registeredAt) | del(.registeredBy)" task-def.json > task-def-updated.json
          
          # Register new task definition
          NEW_TASK_DEF=$(aws ecs register-task-definition --cli-input-json file://task-def-updated.json --query 'taskDefinition.taskDefinitionArn' --output text)
          
          # Update service with new task definition
          aws ecs update-service \
            --cluster $CLUSTER \
            --service $SERVICE \
            --task-definition $NEW_TASK_DEF \
            --force-new-deployment

      - name: Update worker ECS service
        env:
          CLUSTER: nuvana-${{ steps.set-env.outputs.environment }}-cluster
          SERVICE: nuvana-${{ steps.set-env.outputs.environment }}-worker
        run: |
          aws ecs update-service \
            --cluster $CLUSTER \
            --service $SERVICE \
            --force-new-deployment

      - name: Wait for frontend deployment
        env:
          CLUSTER: nuvana-${{ steps.set-env.outputs.environment }}-cluster
          SERVICE: nuvana-${{ steps.set-env.outputs.environment }}-frontend
        timeout-minutes: 5
        continue-on-error: true
        run: |
          echo "Checking frontend service status..."
          # Wait up to 3 minutes for service to stabilize
          # Use timeout to limit wait time, then check if tasks are at least running
          timeout 180 aws ecs wait services-stable \
            --cluster $CLUSTER \
            --services $SERVICE || {
            echo "⚠️ Frontend service did not fully stabilize within 3 minutes, checking task status..."
            RUNNING=$(aws ecs describe-services --cluster $CLUSTER --services $SERVICE --query 'services[0].runningCount' --output text)
            DESIRED=$(aws ecs describe-services --cluster $CLUSTER --services $SERVICE --query 'services[0].desiredCount' --output text)
            echo "Running tasks: $RUNNING / Desired: $DESIRED"
            if [ "$RUNNING" -ge "$DESIRED" ]; then
              echo "✓ Frontend tasks are running ($RUNNING/$DESIRED), deployment may continue despite health check issues"
              exit 0
            else
              echo "✗ Not enough frontend tasks running ($RUNNING/$DESIRED)"
              exit 1
            fi
          }

      - name: Wait for backend deployment
        env:
          CLUSTER: nuvana-${{ steps.set-env.outputs.environment }}-cluster
          SERVICE: nuvana-${{ steps.set-env.outputs.environment }}-backend
        timeout-minutes: 15
        continue-on-error: true
        run: |
          echo "Waiting for backend service to stabilize (with timeout)..."
          # Wait up to 15 minutes for service to stabilize
          # Continue even if it doesn't fully stabilize (tasks may be running but health checks failing)
          timeout 900 aws ecs wait services-stable \
            --cluster $CLUSTER \
            --services $SERVICE || {
            echo "⚠️ Service did not fully stabilize within timeout, but checking status..."
            # Check if tasks are at least running
            RUNNING=$(aws ecs describe-services --cluster $CLUSTER --services $SERVICE --query 'services[0].runningCount' --output text)
            DESIRED=$(aws ecs describe-services --cluster $CLUSTER --services $SERVICE --query 'services[0].desiredCount' --output text)
            echo "Running tasks: $RUNNING / Desired: $DESIRED"
            if [ "$RUNNING" -ge "$DESIRED" ]; then
              echo "✓ Tasks are running, deployment may continue despite health check issues"
              exit 0
            else
              echo "✗ Not enough tasks running"
              exit 1
            fi
          }

      - name: Deployment summary
        run: |
          echo "## Deployment Complete! :rocket:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ steps.set-env.outputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "CI validation was completed on the PR before merge." >> $GITHUB_STEP_SUMMARY
