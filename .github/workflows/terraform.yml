# =============================================================================
# Terraform Infrastructure Pipeline
# =============================================================================
# This workflow:
# 1. Validates Terraform configuration
# 2. Plans infrastructure changes on PR
# 3. Applies infrastructure changes on merge to main
# =============================================================================

name: Terraform Infrastructure

on:
  push:
    branches:
      - main
    paths:
      - 'infrastructure/terraform/**'

  pull_request:
    branches:
      - main
    paths:
      - 'infrastructure/terraform/**'

  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - prod
      action:
        description: 'Terraform action'
        required: true
        default: 'plan'
        type: choice
        options:
          - plan
          - apply
          - destroy

env:
  AWS_REGION: us-east-1
  TF_VERSION: 1.6.0

jobs:
  # ===========================================================================
  # Terraform Validate
  # ===========================================================================
  validate:
    name: Validate
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Format Check
        working-directory: infrastructure/terraform
        run: terraform fmt -check -recursive

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        working-directory: infrastructure/terraform
        run: terraform init -backend=false

      - name: Terraform Validate
        working-directory: infrastructure/terraform
        run: terraform validate

  # ===========================================================================
  # Terraform Plan (Dev)
  # ===========================================================================
  plan-dev:
    name: Plan (Dev)
    runs-on: ubuntu-latest
    needs: validate
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'dev')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        working-directory: infrastructure/terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: infrastructure/terraform
        run: |
          terraform plan \
            -var-file=environments/dev/terraform.tfvars \
            -out=tfplan-dev

      - name: Upload Plan
        uses: actions/upload-artifact@v4
        with:
          name: tfplan-dev
          path: infrastructure/terraform/tfplan-dev

  # ===========================================================================
  # Terraform Apply (Dev)
  # ===========================================================================
  apply-dev:
    name: Apply (Dev)
    runs-on: ubuntu-latest
    needs: plan-dev
    if: |
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' && github.event.inputs.environment == 'dev')
    environment: dev

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        working-directory: infrastructure/terraform
        run: terraform init

      - name: Terraform Apply
        working-directory: infrastructure/terraform
        run: |
          terraform apply \
            -var-file=environments/dev/terraform.tfvars \
            -auto-approve

      - name: Get Outputs
        working-directory: infrastructure/terraform
        run: |
          echo "## Infrastructure Deployed! :cloud:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ALB URL:** $(terraform output -raw application_url)" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Terraform Plan (Prod) - Manual trigger only
  # ===========================================================================
  plan-prod:
    name: Plan (Prod)
    runs-on: ubuntu-latest
    needs: validate
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'prod'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        working-directory: infrastructure/terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: infrastructure/terraform
        run: |
          terraform plan \
            -var-file=environments/prod/terraform.tfvars \
            -out=tfplan-prod

      - name: Upload Plan
        uses: actions/upload-artifact@v4
        with:
          name: tfplan-prod
          path: infrastructure/terraform/tfplan-prod

  # ===========================================================================
  # Terraform Apply (Prod) - Requires approval
  # ===========================================================================
  apply-prod:
    name: Apply (Prod)
    runs-on: ubuntu-latest
    needs: plan-prod
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' && github.event.inputs.environment == 'prod'
    environment: prod  # Requires manual approval

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        working-directory: infrastructure/terraform
        run: terraform init

      - name: Check if MQ and ElastiCache exist in state
        working-directory: infrastructure/terraform
        id: check_resources
        run: |
          # Check if MQ broker and ElastiCache exist in state
          # For downgrade scenarios, we need to destroy and recreate these resources
          echo "Checking if MQ broker and ElastiCache exist in Terraform state..."
          
          if terraform state list | grep -q "module.amazonmq.aws_mq_broker.main"; then
            echo "mq_exists=true" >> $GITHUB_OUTPUT
            echo "✓ MQ broker exists in state"
          else
            echo "mq_exists=false" >> $GITHUB_OUTPUT
            echo "✗ MQ broker does not exist in state"
          fi
          
          if terraform state list | grep -q "module.elasticache.aws_elasticache_cluster.main"; then
            echo "elasticache_exists=true" >> $GITHUB_OUTPUT
            echo "✓ ElastiCache cluster exists in state"
          else
            echo "elasticache_exists=false" >> $GITHUB_OUTPUT
            echo "✗ ElastiCache cluster does not exist in state"
          fi

      - name: Destroy MQ broker and ElastiCache for downgrade
        working-directory: infrastructure/terraform
        id: destroy_resources
        if: steps.check_resources.outputs.mq_exists == 'true' || steps.check_resources.outputs.elasticache_exists == 'true'
        continue-on-error: false
        timeout-minutes: 20
        run: |
          # AWS doesn't allow downgrading instance types - must destroy and recreate
          # Destroy resources first to avoid name conflicts (AWS MQ doesn't allow duplicate names)
          echo "=========================================="
          echo "Destroying resources for downgrade"
          echo "=========================================="
          echo "AWS MQ and ElastiCache cannot be downgraded in-place."
          echo "We must destroy the existing resources and recreate them."
          echo ""
          
          DESTROY_TARGETS=""
          if [ "${{ steps.check_resources.outputs.mq_exists }}" = "true" ]; then
            DESTROY_TARGETS="$DESTROY_TARGETS -target=module.amazonmq.aws_mq_broker.main"
            echo "✓ Will destroy MQ broker"
          fi
          if [ "${{ steps.check_resources.outputs.elasticache_exists }}" = "true" ]; then
            DESTROY_TARGETS="$DESTROY_TARGETS -target=module.elasticache.aws_elasticache_cluster.main"
            echo "✓ Will destroy ElastiCache cluster"
          fi
          
          if [ -z "$DESTROY_TARGETS" ]; then
            echo "No resources to destroy. Skipping destroy step."
            exit 0
          fi
          
          echo ""
          echo "Starting destruction..."
          
          # Retry logic for destroy
          MAX_RETRIES=3
          RETRY_COUNT=0
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if terraform destroy \
              -var-file=environments/prod/terraform.tfvars \
              $DESTROY_TARGETS \
              -auto-approve; then
              echo ""
              echo "✅ Destruction completed successfully"
              exit 0
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                WAIT_TIME=$((2 ** RETRY_COUNT * 60))  # 60s, 120s, 240s
                echo ""
                echo "⚠️  Destruction failed. Retrying in ${WAIT_TIME} seconds... (Attempt $RETRY_COUNT/$MAX_RETRIES)"
                sleep $WAIT_TIME
                echo "Re-initializing Terraform..."
                terraform init -reconfigure
              fi
            fi
          done
          echo ""
          echo "❌ Destruction failed after $MAX_RETRIES attempts"
          exit 1

      - name: Remove destroyed resources from state
        working-directory: infrastructure/terraform
        if: steps.destroy_resources.conclusion == 'success'
        run: |
          # Remove destroyed resources from state to force Terraform to treat them as new
          # This prevents Terraform from trying to do create-before-destroy
          echo "Removing destroyed resources from Terraform state..."
          if [ "${{ steps.check_resources.outputs.mq_exists }}" = "true" ]; then
            terraform state rm module.amazonmq.aws_mq_broker.main || true
            echo "✓ Removed MQ broker from state"
          fi
          if [ "${{ steps.check_resources.outputs.elasticache_exists }}" = "true" ]; then
            terraform state rm module.elasticache.aws_elasticache_cluster.main || true
            echo "✓ Removed ElastiCache cluster from state"
          fi

      - name: Wait for AWS resource cleanup
        working-directory: infrastructure/terraform
        if: steps.destroy_resources.conclusion == 'success'
        run: |
          # AWS needs time to fully clean up destroyed resources
          # MQ broker names are globally unique and must be fully deleted before reuse
          echo "Waiting for AWS to complete resource cleanup..."
          echo "MQ broker names are globally unique - must wait for full deletion."
          echo "Waiting 90 seconds to ensure resources are fully removed..."
          sleep 90
          echo "✓ Cleanup wait complete - resources should now be available for recreation"

      - name: Terraform Apply (complete configuration)
        working-directory: infrastructure/terraform
        id: terraform_apply
        continue-on-error: false
        timeout-minutes: 30
        run: |
          # Final apply to ensure all resources are in the desired state
          # Retry logic: try up to 3 times with exponential backoff
          MAX_RETRIES=3
          RETRY_COUNT=0
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if terraform apply \
              -var-file=environments/prod/terraform.tfvars \
              -auto-approve; then
              echo "Apply completed successfully"
              exit 0
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                WAIT_TIME=$((2 ** RETRY_COUNT * 60))  # 60s, 120s, 240s
                echo "Apply failed. Retrying in ${WAIT_TIME} seconds... (Attempt $RETRY_COUNT/$MAX_RETRIES)"
                sleep $WAIT_TIME
                # Re-initialize in case of state issues
                terraform init -reconfigure
              fi
            fi
          done
          echo "Apply failed after $MAX_RETRIES attempts"
          exit 1

      - name: Get Outputs
        working-directory: infrastructure/terraform
        run: |
          echo "## Production Infrastructure Deployed! :rocket:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ALB URL:** $(terraform output -raw application_url)" >> $GITHUB_STEP_SUMMARY
